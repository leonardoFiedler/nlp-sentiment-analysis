{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kK1xdmK3cL0Z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "1CZbMdlecL0r",
    "outputId": "a8b5c027-2d0c-475f-c9ad-54c1e8fcd01f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>watch</th>\n",
       "      <th>episode</th>\n",
       "      <th>right</th>\n",
       "      <th>exactly</th>\n",
       "      <th>happen</th>\n",
       "      <th>thing</th>\n",
       "      <th>scene</th>\n",
       "      <th>violence</th>\n",
       "      <th>set</th>\n",
       "      <th>word</th>\n",
       "      <th>pull</th>\n",
       "      <th>drug</th>\n",
       "      <th>sex</th>\n",
       "      <th>classic</th>\n",
       "      <th>use</th>\n",
       "      <th>state</th>\n",
       "      <th>focus</th>\n",
       "      <th>city</th>\n",
       "      <th>face</th>\n",
       "      <th>high</th>\n",
       "      <th>home</th>\n",
       "      <th>italian</th>\n",
       "      <th>death</th>\n",
       "      <th>far</th>\n",
       "      <th>away</th>\n",
       "      <th>main</th>\n",
       "      <th>appeal</th>\n",
       "      <th>fact</th>\n",
       "      <th>forget</th>\n",
       "      <th>pretty</th>\n",
       "      <th>picture</th>\n",
       "      <th>audience</th>\n",
       "      <th>romance</th>\n",
       "      <th>mess</th>\n",
       "      <th>develop</th>\n",
       "      <th>taste</th>\n",
       "      <th>level</th>\n",
       "      <th>sell</th>\n",
       "      <th>kill</th>\n",
       "      <th>...</th>\n",
       "      <th>crew</th>\n",
       "      <th>history</th>\n",
       "      <th>jane</th>\n",
       "      <th>copy</th>\n",
       "      <th>realistic</th>\n",
       "      <th>chase</th>\n",
       "      <th>location</th>\n",
       "      <th>choice</th>\n",
       "      <th>footage</th>\n",
       "      <th>blue</th>\n",
       "      <th>badly</th>\n",
       "      <th>cat</th>\n",
       "      <th>sci</th>\n",
       "      <th>fi</th>\n",
       "      <th>sexual</th>\n",
       "      <th>escape</th>\n",
       "      <th>road</th>\n",
       "      <th>scream</th>\n",
       "      <th>attention</th>\n",
       "      <th>t</th>\n",
       "      <th>step</th>\n",
       "      <th>society</th>\n",
       "      <th>recently</th>\n",
       "      <th>adaptation</th>\n",
       "      <th>powerful</th>\n",
       "      <th>party</th>\n",
       "      <th>park</th>\n",
       "      <th>portrayal</th>\n",
       "      <th>science</th>\n",
       "      <th>search</th>\n",
       "      <th>basic</th>\n",
       "      <th>vampire</th>\n",
       "      <th>color</th>\n",
       "      <th>maker</th>\n",
       "      <th>channel</th>\n",
       "      <th>culture</th>\n",
       "      <th>dramatic</th>\n",
       "      <th>intelligent</th>\n",
       "      <th>scott</th>\n",
       "      <th>sentiment_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 854 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mention  watch  episode  ...  intelligent  scott  sentiment_target\n",
       "0        1      3        2  ...            0      0          positive\n",
       "1        0      0        0  ...            0      0          positive\n",
       "2        0      1        0  ...            0      0          positive\n",
       "3        0      1        0  ...            0      0          negative\n",
       "4        0      1        0  ...            0      0          positive\n",
       "\n",
       "[5 rows x 854 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_10k = pd.read_csv('./bag_of_words_full.csv', index_col=0)\n",
    "df_10k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "TSd-NOFKcL0w",
    "outputId": "5471ead9-d6e7-490c-8546-f20cf66872bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>watch</th>\n",
       "      <th>episode</th>\n",
       "      <th>right</th>\n",
       "      <th>exactly</th>\n",
       "      <th>happen</th>\n",
       "      <th>thing</th>\n",
       "      <th>scene</th>\n",
       "      <th>violence</th>\n",
       "      <th>set</th>\n",
       "      <th>word</th>\n",
       "      <th>pull</th>\n",
       "      <th>drug</th>\n",
       "      <th>sex</th>\n",
       "      <th>classic</th>\n",
       "      <th>use</th>\n",
       "      <th>state</th>\n",
       "      <th>focus</th>\n",
       "      <th>city</th>\n",
       "      <th>face</th>\n",
       "      <th>high</th>\n",
       "      <th>home</th>\n",
       "      <th>italian</th>\n",
       "      <th>death</th>\n",
       "      <th>far</th>\n",
       "      <th>away</th>\n",
       "      <th>main</th>\n",
       "      <th>appeal</th>\n",
       "      <th>fact</th>\n",
       "      <th>forget</th>\n",
       "      <th>pretty</th>\n",
       "      <th>picture</th>\n",
       "      <th>audience</th>\n",
       "      <th>romance</th>\n",
       "      <th>mess</th>\n",
       "      <th>develop</th>\n",
       "      <th>taste</th>\n",
       "      <th>level</th>\n",
       "      <th>sell</th>\n",
       "      <th>kill</th>\n",
       "      <th>...</th>\n",
       "      <th>answer</th>\n",
       "      <th>crew</th>\n",
       "      <th>history</th>\n",
       "      <th>jane</th>\n",
       "      <th>copy</th>\n",
       "      <th>realistic</th>\n",
       "      <th>chase</th>\n",
       "      <th>location</th>\n",
       "      <th>choice</th>\n",
       "      <th>footage</th>\n",
       "      <th>blue</th>\n",
       "      <th>badly</th>\n",
       "      <th>cat</th>\n",
       "      <th>sci</th>\n",
       "      <th>fi</th>\n",
       "      <th>sexual</th>\n",
       "      <th>escape</th>\n",
       "      <th>road</th>\n",
       "      <th>scream</th>\n",
       "      <th>attention</th>\n",
       "      <th>t</th>\n",
       "      <th>step</th>\n",
       "      <th>society</th>\n",
       "      <th>recently</th>\n",
       "      <th>adaptation</th>\n",
       "      <th>powerful</th>\n",
       "      <th>party</th>\n",
       "      <th>park</th>\n",
       "      <th>portrayal</th>\n",
       "      <th>science</th>\n",
       "      <th>search</th>\n",
       "      <th>basic</th>\n",
       "      <th>vampire</th>\n",
       "      <th>color</th>\n",
       "      <th>maker</th>\n",
       "      <th>channel</th>\n",
       "      <th>culture</th>\n",
       "      <th>dramatic</th>\n",
       "      <th>intelligent</th>\n",
       "      <th>scott</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.0000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.060760</td>\n",
       "      <td>0.554300</td>\n",
       "      <td>0.098740</td>\n",
       "      <td>0.137980</td>\n",
       "      <td>0.039280</td>\n",
       "      <td>0.141620</td>\n",
       "      <td>0.329960</td>\n",
       "      <td>0.428340</td>\n",
       "      <td>0.042420</td>\n",
       "      <td>0.136560</td>\n",
       "      <td>0.074120</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.03626</td>\n",
       "      <td>0.069540</td>\n",
       "      <td>0.081340</td>\n",
       "      <td>0.092460</td>\n",
       "      <td>0.041860</td>\n",
       "      <td>0.036640</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.090160</td>\n",
       "      <td>0.100920</td>\n",
       "      <td>0.076440</td>\n",
       "      <td>0.02406</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.120800</td>\n",
       "      <td>0.11036</td>\n",
       "      <td>0.092260</td>\n",
       "      <td>0.022880</td>\n",
       "      <td>0.147020</td>\n",
       "      <td>0.053860</td>\n",
       "      <td>0.145560</td>\n",
       "      <td>0.076460</td>\n",
       "      <td>0.104660</td>\n",
       "      <td>0.02976</td>\n",
       "      <td>0.029540</td>\n",
       "      <td>0.032040</td>\n",
       "      <td>0.021440</td>\n",
       "      <td>0.048020</td>\n",
       "      <td>0.022720</td>\n",
       "      <td>0.135920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024660</td>\n",
       "      <td>0.02786</td>\n",
       "      <td>0.052060</td>\n",
       "      <td>0.02104</td>\n",
       "      <td>0.031980</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.031940</td>\n",
       "      <td>0.026220</td>\n",
       "      <td>0.02768</td>\n",
       "      <td>0.027040</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.025620</td>\n",
       "      <td>0.024640</td>\n",
       "      <td>0.028160</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.026820</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.020260</td>\n",
       "      <td>0.02506</td>\n",
       "      <td>0.036660</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.022380</td>\n",
       "      <td>0.028440</td>\n",
       "      <td>0.022920</td>\n",
       "      <td>0.020180</td>\n",
       "      <td>0.024680</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.02028</td>\n",
       "      <td>0.023380</td>\n",
       "      <td>0.021760</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.020540</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.023460</td>\n",
       "      <td>0.02544</td>\n",
       "      <td>0.02366</td>\n",
       "      <td>0.022180</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.020640</td>\n",
       "      <td>0.021640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.274426</td>\n",
       "      <td>0.910773</td>\n",
       "      <td>0.563877</td>\n",
       "      <td>0.422167</td>\n",
       "      <td>0.210945</td>\n",
       "      <td>0.441234</td>\n",
       "      <td>0.699611</td>\n",
       "      <td>0.923299</td>\n",
       "      <td>0.255933</td>\n",
       "      <td>0.422889</td>\n",
       "      <td>0.304939</td>\n",
       "      <td>0.208013</td>\n",
       "      <td>0.30415</td>\n",
       "      <td>0.381297</td>\n",
       "      <td>0.323737</td>\n",
       "      <td>0.341341</td>\n",
       "      <td>0.241555</td>\n",
       "      <td>0.217574</td>\n",
       "      <td>0.304026</td>\n",
       "      <td>0.348014</td>\n",
       "      <td>0.375044</td>\n",
       "      <td>0.327168</td>\n",
       "      <td>0.22619</td>\n",
       "      <td>0.386694</td>\n",
       "      <td>0.388292</td>\n",
       "      <td>0.36357</td>\n",
       "      <td>0.351099</td>\n",
       "      <td>0.164673</td>\n",
       "      <td>0.440374</td>\n",
       "      <td>0.248275</td>\n",
       "      <td>0.456307</td>\n",
       "      <td>0.336179</td>\n",
       "      <td>0.394245</td>\n",
       "      <td>0.20336</td>\n",
       "      <td>0.183815</td>\n",
       "      <td>0.195075</td>\n",
       "      <td>0.159565</td>\n",
       "      <td>0.252973</td>\n",
       "      <td>0.172408</td>\n",
       "      <td>0.508341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193009</td>\n",
       "      <td>0.21458</td>\n",
       "      <td>0.271645</td>\n",
       "      <td>0.31591</td>\n",
       "      <td>0.209089</td>\n",
       "      <td>0.196119</td>\n",
       "      <td>0.230653</td>\n",
       "      <td>0.175651</td>\n",
       "      <td>0.18065</td>\n",
       "      <td>0.221517</td>\n",
       "      <td>0.205255</td>\n",
       "      <td>0.180012</td>\n",
       "      <td>0.241649</td>\n",
       "      <td>0.226292</td>\n",
       "      <td>0.226537</td>\n",
       "      <td>0.202735</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.169027</td>\n",
       "      <td>0.19275</td>\n",
       "      <td>0.201288</td>\n",
       "      <td>0.203543</td>\n",
       "      <td>0.182647</td>\n",
       "      <td>0.203941</td>\n",
       "      <td>0.155162</td>\n",
       "      <td>0.178699</td>\n",
       "      <td>0.176385</td>\n",
       "      <td>0.213648</td>\n",
       "      <td>0.20127</td>\n",
       "      <td>0.166715</td>\n",
       "      <td>0.196894</td>\n",
       "      <td>0.161461</td>\n",
       "      <td>0.154527</td>\n",
       "      <td>0.358423</td>\n",
       "      <td>0.186414</td>\n",
       "      <td>0.18120</td>\n",
       "      <td>0.18028</td>\n",
       "      <td>0.184198</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.154449</td>\n",
       "      <td>0.236501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 853 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mention         watch  ...   intelligent         scott\n",
       "count  50000.000000  50000.000000  ...  50000.000000  50000.000000\n",
       "mean       0.060760      0.554300  ...      0.020640      0.021640\n",
       "std        0.274426      0.910773  ...      0.154449      0.236501\n",
       "min        0.000000      0.000000  ...      0.000000      0.000000\n",
       "25%        0.000000      0.000000  ...      0.000000      0.000000\n",
       "50%        0.000000      0.000000  ...      0.000000      0.000000\n",
       "75%        0.000000      1.000000  ...      0.000000      0.000000\n",
       "max        8.000000     12.000000  ...      4.000000     12.000000\n",
       "\n",
       "[8 rows x 853 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_10k.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "Emt551bucL0x",
    "outputId": "fb639117-aace-4b56-8ea2-fefc9edd1291"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f79260b7f90>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_10k.sentiment_target.value_counts().plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "g5suVpufcL0x",
    "outputId": "689d2a0b-eaf5-4306-b999-a5831c85b6fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>watch</th>\n",
       "      <th>episode</th>\n",
       "      <th>right</th>\n",
       "      <th>exactly</th>\n",
       "      <th>happen</th>\n",
       "      <th>thing</th>\n",
       "      <th>scene</th>\n",
       "      <th>violence</th>\n",
       "      <th>set</th>\n",
       "      <th>word</th>\n",
       "      <th>pull</th>\n",
       "      <th>drug</th>\n",
       "      <th>sex</th>\n",
       "      <th>classic</th>\n",
       "      <th>use</th>\n",
       "      <th>state</th>\n",
       "      <th>focus</th>\n",
       "      <th>city</th>\n",
       "      <th>face</th>\n",
       "      <th>high</th>\n",
       "      <th>home</th>\n",
       "      <th>italian</th>\n",
       "      <th>death</th>\n",
       "      <th>far</th>\n",
       "      <th>away</th>\n",
       "      <th>main</th>\n",
       "      <th>appeal</th>\n",
       "      <th>fact</th>\n",
       "      <th>forget</th>\n",
       "      <th>pretty</th>\n",
       "      <th>picture</th>\n",
       "      <th>audience</th>\n",
       "      <th>romance</th>\n",
       "      <th>mess</th>\n",
       "      <th>develop</th>\n",
       "      <th>taste</th>\n",
       "      <th>level</th>\n",
       "      <th>sell</th>\n",
       "      <th>kill</th>\n",
       "      <th>...</th>\n",
       "      <th>answer</th>\n",
       "      <th>crew</th>\n",
       "      <th>history</th>\n",
       "      <th>jane</th>\n",
       "      <th>copy</th>\n",
       "      <th>realistic</th>\n",
       "      <th>chase</th>\n",
       "      <th>location</th>\n",
       "      <th>choice</th>\n",
       "      <th>footage</th>\n",
       "      <th>blue</th>\n",
       "      <th>badly</th>\n",
       "      <th>cat</th>\n",
       "      <th>sci</th>\n",
       "      <th>fi</th>\n",
       "      <th>sexual</th>\n",
       "      <th>escape</th>\n",
       "      <th>road</th>\n",
       "      <th>scream</th>\n",
       "      <th>attention</th>\n",
       "      <th>t</th>\n",
       "      <th>step</th>\n",
       "      <th>society</th>\n",
       "      <th>recently</th>\n",
       "      <th>adaptation</th>\n",
       "      <th>powerful</th>\n",
       "      <th>party</th>\n",
       "      <th>park</th>\n",
       "      <th>portrayal</th>\n",
       "      <th>science</th>\n",
       "      <th>search</th>\n",
       "      <th>basic</th>\n",
       "      <th>vampire</th>\n",
       "      <th>color</th>\n",
       "      <th>maker</th>\n",
       "      <th>channel</th>\n",
       "      <th>culture</th>\n",
       "      <th>dramatic</th>\n",
       "      <th>intelligent</th>\n",
       "      <th>scott</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.00000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.00000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.00000</td>\n",
       "      <td>30000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.553433</td>\n",
       "      <td>0.097767</td>\n",
       "      <td>0.138667</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.142233</td>\n",
       "      <td>0.327500</td>\n",
       "      <td>0.422967</td>\n",
       "      <td>0.042733</td>\n",
       "      <td>0.137967</td>\n",
       "      <td>0.073933</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>0.036167</td>\n",
       "      <td>0.069900</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.092833</td>\n",
       "      <td>0.040933</td>\n",
       "      <td>0.035667</td>\n",
       "      <td>0.051533</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.100500</td>\n",
       "      <td>0.073867</td>\n",
       "      <td>0.023867</td>\n",
       "      <td>0.083900</td>\n",
       "      <td>0.121967</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>0.093933</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.144767</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.074733</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.029867</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.048467</td>\n",
       "      <td>0.022633</td>\n",
       "      <td>0.135833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023767</td>\n",
       "      <td>0.028167</td>\n",
       "      <td>0.052233</td>\n",
       "      <td>0.022967</td>\n",
       "      <td>0.031933</td>\n",
       "      <td>0.031833</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.025967</td>\n",
       "      <td>0.026867</td>\n",
       "      <td>0.025467</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.02550</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.019533</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.036833</td>\n",
       "      <td>0.022567</td>\n",
       "      <td>0.022267</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.021867</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.022833</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.023067</td>\n",
       "      <td>0.021267</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.020233</td>\n",
       "      <td>0.029200</td>\n",
       "      <td>0.022733</td>\n",
       "      <td>0.026033</td>\n",
       "      <td>0.022367</td>\n",
       "      <td>0.023367</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.02030</td>\n",
       "      <td>0.022433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.275395</td>\n",
       "      <td>0.908206</td>\n",
       "      <td>0.554694</td>\n",
       "      <td>0.426432</td>\n",
       "      <td>0.213835</td>\n",
       "      <td>0.439783</td>\n",
       "      <td>0.697754</td>\n",
       "      <td>0.921174</td>\n",
       "      <td>0.257377</td>\n",
       "      <td>0.424584</td>\n",
       "      <td>0.305183</td>\n",
       "      <td>0.206697</td>\n",
       "      <td>0.325365</td>\n",
       "      <td>0.372763</td>\n",
       "      <td>0.324817</td>\n",
       "      <td>0.342373</td>\n",
       "      <td>0.226994</td>\n",
       "      <td>0.215707</td>\n",
       "      <td>0.310828</td>\n",
       "      <td>0.349852</td>\n",
       "      <td>0.366339</td>\n",
       "      <td>0.318978</td>\n",
       "      <td>0.226050</td>\n",
       "      <td>0.377093</td>\n",
       "      <td>0.387938</td>\n",
       "      <td>0.367113</td>\n",
       "      <td>0.360436</td>\n",
       "      <td>0.170814</td>\n",
       "      <td>0.434376</td>\n",
       "      <td>0.246433</td>\n",
       "      <td>0.45645</td>\n",
       "      <td>0.332694</td>\n",
       "      <td>0.398245</td>\n",
       "      <td>0.203738</td>\n",
       "      <td>0.182679</td>\n",
       "      <td>0.188635</td>\n",
       "      <td>0.154843</td>\n",
       "      <td>0.259589</td>\n",
       "      <td>0.172980</td>\n",
       "      <td>0.518451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177959</td>\n",
       "      <td>0.216737</td>\n",
       "      <td>0.273692</td>\n",
       "      <td>0.337307</td>\n",
       "      <td>0.210195</td>\n",
       "      <td>0.200719</td>\n",
       "      <td>0.220281</td>\n",
       "      <td>0.173474</td>\n",
       "      <td>0.179664</td>\n",
       "      <td>0.211391</td>\n",
       "      <td>0.215309</td>\n",
       "      <td>0.175488</td>\n",
       "      <td>0.25466</td>\n",
       "      <td>0.229336</td>\n",
       "      <td>0.230250</td>\n",
       "      <td>0.209176</td>\n",
       "      <td>0.225254</td>\n",
       "      <td>0.164983</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.201357</td>\n",
       "      <td>0.206540</td>\n",
       "      <td>0.186830</td>\n",
       "      <td>0.199041</td>\n",
       "      <td>0.151842</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.167072</td>\n",
       "      <td>0.203048</td>\n",
       "      <td>0.196675</td>\n",
       "      <td>0.164729</td>\n",
       "      <td>0.187124</td>\n",
       "      <td>0.161545</td>\n",
       "      <td>0.152397</td>\n",
       "      <td>0.361965</td>\n",
       "      <td>0.184980</td>\n",
       "      <td>0.188034</td>\n",
       "      <td>0.172629</td>\n",
       "      <td>0.193275</td>\n",
       "      <td>0.181597</td>\n",
       "      <td>0.15107</td>\n",
       "      <td>0.255993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>14.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 853 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mention         watch  ...  intelligent         scott\n",
       "count  30000.000000  30000.000000  ...  30000.00000  30000.000000\n",
       "mean       0.060500      0.553433  ...      0.02030      0.022433\n",
       "std        0.275395      0.908206  ...      0.15107      0.255993\n",
       "min        0.000000      0.000000  ...      0.00000      0.000000\n",
       "25%        0.000000      0.000000  ...      0.00000      0.000000\n",
       "50%        0.000000      0.000000  ...      0.00000      0.000000\n",
       "75%        0.000000      1.000000  ...      0.00000      0.000000\n",
       "max        8.000000     12.000000  ...      3.00000     12.000000\n",
       "\n",
       "[8 rows x 853 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 15000\n",
    "df_positive = df_10k[df_10k['sentiment_target'] == 'positive'].sample(n=n)\n",
    "df_negative = df_10k[df_10k['sentiment_target'] == 'negative'].sample(n=n)\n",
    "\n",
    "df_sample_test = df_positive.append(df_negative)\n",
    "df_sample_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "6U_WN4XHcL0z",
    "outputId": "1c033cac-26e1-4cff-a43a-4d00494eff7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7926155e10>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sample_test.sentiment_target.value_counts().plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAPn7POTqGNk"
   },
   "source": [
    "## SIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2EJBg_8tGHW",
    "outputId": "6aaa2602-766f-46b8-aab6-c40f16f79820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "wXlUMejIssOS",
    "outputId": "9d5cb208-113e-465a-dd7b-5744930eb475"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews = pd.read_csv('./IMDB Dataset.csv')\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "52HBYWCnyNMd",
    "outputId": "18f94415-cf4e-4d10-8b38-79ba82239958"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17144</th>\n",
       "      <td>The main problem I see with this film is its s...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49659</th>\n",
       "      <td>I saw Beyond Rangoon about 20 times, it was TH...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35966</th>\n",
       "      <td>It has been about 50 years since a movie has b...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35534</th>\n",
       "      <td>I was delighted when I saw that my husband ren...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27358</th>\n",
       "      <td>It's a movie with a theatrical message blended...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "17144  The main problem I see with this film is its s...  positive\n",
       "49659  I saw Beyond Rangoon about 20 times, it was TH...  positive\n",
       "35966  It has been about 50 years since a movie has b...  positive\n",
       "35534  I was delighted when I saw that my husband ren...  positive\n",
       "27358  It's a movie with a theatrical message blended...  positive"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews_test = df_reviews.iloc[df_sample_test.index]\n",
    "df_reviews_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "L4JZR9qVzDhO",
    "outputId": "0caf31f0-1542-46b2-dfbd-c958297f5633"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f790b5dbd90>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_reviews_test.sentiment.value_counts().plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1PYQkEp4xdX7"
   },
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "corrects = 0\n",
    "\n",
    "for index, row in df_reviews_test.iterrows():\n",
    "    sentiment_dict = sia.polarity_scores(row['review'])\n",
    "\n",
    "    # Ideia da analise https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
    "    if (sentiment_dict['compound'] >= 0.05 and row['sentiment'] == 'positive') or (sentiment_dict['compound'] <= - 0.05 and row['sentiment'] == 'negative'): \n",
    "        corrects += 1\n",
    "\n",
    "acuracia = corrects/df_reviews_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OicyDSVJ5Fok",
    "outputId": "2665b378-b2df-4913-bc02-39916415b0a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corretos: 20746\n",
      "Total: 30000\n",
      "Acurácia: 0.6915333333333333\n"
     ]
    }
   ],
   "source": [
    "print(f\"Corretos: {corrects}\")\n",
    "print(f\"Total: {df_reviews_test.shape[0]}\")\n",
    "print(f\"Acurácia: {acuracia}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5w1bp3zqGNl"
   },
   "source": [
    "## Classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqRQqdaLcL04"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSekdQEAh8e-",
    "outputId": "00a25233-780c-4a2a-c566-92039965dfb4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "--------------------------------------------------\n",
      "logistic_regression\n",
      "--------------------------------------------------\n",
      "Train - Accuracy: 0.8644333333333334\n",
      "Train - Precision: 0.853277569694183\n",
      "Train - Recall: 0.880251851851852\n",
      "Train - Fscore: 0.8665497361716412\n",
      "--------------------------------------------------\n",
      "Test - Accuracy: 0.851\n",
      "Test - Precision: 0.8405017948165039\n",
      "Test - Recall: 0.8664666666666667\n",
      "Test - Fscore: 0.8532738359560665\n",
      "\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "[10:58:36] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "[11:10:00] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "[11:20:50] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "[11:31:59] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "[11:43:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "[11:54:39] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "[12:05:48] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "[12:16:54] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "[12:27:49] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "[12:39:10] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "--------------------------------------------------\n",
      "xgboost\n",
      "--------------------------------------------------\n",
      "Train - Accuracy: 0.9189259259259259\n",
      "Train - Precision: 0.9039895699911117\n",
      "Train - Recall: 0.9374148148148149\n",
      "Train - Fscore: 0.9203979438411685\n",
      "--------------------------------------------------\n",
      "Test - Accuracy: 0.8279666666666665\n",
      "Test - Precision: 0.8169500726728375\n",
      "Test - Recall: 0.8454666666666666\n",
      "Test - Fscore: 0.8309464314954284\n",
      "\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "--------------------------------------------------\n",
      "mlp\n",
      "--------------------------------------------------\n",
      "Train - Accuracy: 0.998948148148148\n",
      "Train - Precision: 0.9990142504608055\n",
      "Train - Recall: 0.9988814814814815\n",
      "Train - Fscore: 0.9989476337918202\n",
      "--------------------------------------------------\n",
      "Test - Accuracy: 0.7951333333333334\n",
      "Test - Precision: 0.7916816834940178\n",
      "Test - Recall: 0.8012666666666668\n",
      "Test - Fscore: 0.7964063442145081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def return_features(df_train, df_test):\n",
    "    oe = OrdinalEncoder()\n",
    "\n",
    "    y_train = oe.fit_transform(df_train.sentiment_target.values.reshape(-1, 1))\n",
    "    y_test = oe.transform(df_test.sentiment_target.values.reshape(-1, 1))\n",
    "    \n",
    "    X_train = df_train.drop('sentiment_target', axis=1)\n",
    "    X_test = df_test.drop('sentiment_target', axis=1)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def choose_model(alg):\n",
    "    if alg == 'logistic_regression':\n",
    "        return GridSearchCV(\n",
    "            LogisticRegression(n_jobs=-1),\n",
    "            param_grid={\n",
    "                'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                'class_weight': [None, 'balanced']\n",
    "            },\n",
    "            scoring='f1',\n",
    "            cv=StratifiedKFold(5),\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "    elif alg == 'svc':\n",
    "        return GridSearchCV(\n",
    "            SVC(),\n",
    "            param_grid={\n",
    "                'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                'kernel': ['poly', 'rbf'],\n",
    "                'gamma': ['scale', 'auto'],\n",
    "            },\n",
    "            scoring='f1',\n",
    "            cv=StratifiedKFold(5),\n",
    "            verbose=1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    elif alg == 'svc_linear':\n",
    "        return GridSearchCV(\n",
    "            LinearSVC(),\n",
    "            param_grid={\n",
    "                'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'class_weight': [None, 'balanced'],\n",
    "                'loss': ['hinge', 'squared_hinge'],\n",
    "                'dual': [True, False]\n",
    "            },\n",
    "            scoring='f1',\n",
    "            cv=StratifiedKFold(5),\n",
    "            verbose=1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    elif alg == 'xgboost':\n",
    "        return GridSearchCV(\n",
    "            XGBClassifier(n_estimators=50),\n",
    "            param_grid={\n",
    "                'max_depth': [None, 4, 8],\n",
    "                'scale_pos_weight': [1] # neg / pos\n",
    "            },\n",
    "            scoring='f1',\n",
    "            cv=StratifiedKFold(5),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    elif alg == 'mlp':\n",
    "        return GridSearchCV(\n",
    "            MLPClassifier(),\n",
    "            param_grid={\n",
    "                'solver': ['lbfgs'],\n",
    "                'max_iter': [1000, 2000],\n",
    "                'hidden_layer_sizes': np.arange(10, 15)\n",
    "            },\n",
    "            scoring='f1',\n",
    "            cv=StratifiedKFold(5),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "# 'svc', 'svc_linear',\n",
    "for alg in ['logistic_regression', 'xgboost', 'mlp']:\n",
    "    folds = 10\n",
    "\n",
    "    cv = StratifiedKFold(folds, shuffle=True, random_state=42)\n",
    "    metrics_train = np.zeros(shape=(folds, 4))\n",
    "    metrics_test = np.zeros(shape=(folds, 4))\n",
    "\n",
    "    for fold, (train, test) in enumerate(cv.split(df_sample_test, df_sample_test.sentiment_target.values)):\n",
    "        df_train = df_sample_test.iloc[train]\n",
    "        df_test = df_sample_test.iloc[test]\n",
    "\n",
    "        X_train, y_train, X_test, y_test = return_features(df_train, df_test)\n",
    "        \n",
    "        model = choose_model(alg)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        accuracy = accuracy_score(y_train, y_pred_train)\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(y_train, y_pred_train, average='binary')\n",
    "        metrics_train[fold, 0] = accuracy\n",
    "        metrics_train[fold, 1] = precision\n",
    "        metrics_train[fold, 2] = recall\n",
    "        metrics_train[fold, 3] = fscore\n",
    "\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred_test)\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred_test, average='binary')\n",
    "        metrics_test[fold, 0] = accuracy\n",
    "        metrics_test[fold, 1] = precision\n",
    "        metrics_test[fold, 2] = recall\n",
    "        metrics_test[fold, 3] = fscore\n",
    "    \n",
    "    print('-' * 50)\n",
    "    print(alg)\n",
    "    print('-' * 50)\n",
    "    print('Train - Accuracy:', metrics_train[:,0].mean())\n",
    "    print('Train - Precision:', metrics_train[:,1].mean())\n",
    "    print('Train - Recall:', metrics_train[:,2].mean())\n",
    "    print('Train - Fscore:', metrics_train[:,3].mean())\n",
    "    print('-' * 50)\n",
    "    print('Test - Accuracy:', metrics_test[:,0].mean())\n",
    "    print('Test - Precision:', metrics_test[:,1].mean())\n",
    "    print('Test - Recall:', metrics_test[:,2].mean())\n",
    "    print('Test - Fscore:', metrics_test[:,3].mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyj_ja-4iPbQ",
    "outputId": "cefa52f1-8963-484d-949d-6656f76e90b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_layer_sizes': 13, 'max_iter': 1000, 'solver': 'lbfgs'}"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfGkOnYLkT6M",
    "outputId": "99231185-6b02-4425-c0d6-5a30cc973906"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/leonardofiedler/Desenvolvimento/nlp-sentiment-analysis/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "[22:44:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=8,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = XGBClassifier(max_depth=8, scale_pos_weight=1)\n",
    "modelo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9mIQJHSkp2B",
    "outputId": "3f441073-dfdd-4a03-daab-96d4baadfb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.78      0.81       988\n",
      "         1.0       0.80      0.86      0.83      1012\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.82      0.82      0.82      2000\n",
      "weighted avg       0.82      0.82      0.82      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_final = model.predict(X_test)\n",
    "print(classification_report(y_test, predicted_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjANu9GVxGBU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "analysis_10k.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
